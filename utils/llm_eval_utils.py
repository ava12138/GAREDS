import random
import re
import json
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
import time
import os
import threading

# =====================
# å¤§æ¨¡å‹è¯„ä¼°å·¥å…·å‡½æ•°
# =====================

def llm_mcq_judge(client, api_model, question, correct_answer, distractors, max_retries=2):
    """
    è®©å¤§æ¨¡å‹åœ¨å¹²æ‰°é¡¹å’Œæ­£ç¡®ç­”æ¡ˆä¸­é€‰æ‹©ï¼Œè¿”å›æ˜¯å¦é€‰å¯¹ã€‚
    è‡ªåŠ¨å…¼å®¹æµå¼å’Œéæµå¼APIã€‚
    
    Args:
        client: OpenAIå…¼å®¹çš„APIå®¢æˆ·ç«¯
        api_model: æ¨¡å‹åç§°
        question: é—®é¢˜æ–‡æœ¬
        correct_answer: æ­£ç¡®ç­”æ¡ˆ
        distractors: å¹²æ‰°é¡¹åˆ—è¡¨
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        
    Returns:
        tuple: (æ˜¯å¦é€‰å¯¹, è¾“å…¥tokenæ•°, è¾“å‡ºtokenæ•°)
    """
    # æ„å»ºå¤šé€‰é¢˜promptï¼Œå°†æ­£ç¡®ç­”æ¡ˆå’Œå¹²æ‰°é¡¹éšæœºæ’åˆ—
    options = distractors + [correct_answer]
    random.shuffle(options)
    option_str = "\n".join([f"{chr(65+i)}. {opt}" for i, opt in enumerate(options)])
    prompt = f"Question: {question}\nOptions:\n{option_str}\nPlease select the MOST correct answer, and only return the letter of the option."
    correct_index = options.index(correct_answer)
    
    retries = 0
    while retries <= max_retries:
        try:
            # ä¼˜å…ˆå°è¯•éæµå¼
            input_tokens = 0
            output_tokens = 0
            try:
                response = client.chat.completions.create(
                    model=api_model,
                    messages=[{"role": "user", "content": prompt}],
                    temperature=0.1,
                    stream=False
                )
                answer = response.choices[0].message.content.strip().upper()
                if hasattr(response, 'usage'):
                    if hasattr(response.usage, 'prompt_tokens'):
                        input_tokens = response.usage.prompt_tokens
                    if hasattr(response.usage, 'completion_tokens'):
                        output_tokens = response.usage.completion_tokens
                    # å¦‚æœåªæœ‰total_tokensä½†æ²¡æœ‰ç»†åˆ†ï¼Œè¿›è¡Œä¼°ç®—
                    if hasattr(response.usage, 'total_tokens') and (input_tokens == 0 or output_tokens == 0):
                        input_tokens = int(response.usage.total_tokens * 0.8)  # ä¼°ç®—è¾“å…¥å 80%
                        output_tokens = response.usage.total_tokens - input_tokens  # ä¼°ç®—è¾“å‡ºå 20%
            except Exception as e:
                # æ£€æŸ¥æ˜¯å¦éœ€è¦æµå¼
                if hasattr(e, 'response') and hasattr(e.response, 'status_code') and e.response.status_code == 400:
                    err_msg = str(e)
                    if 'only support stream mode' in err_msg or 'please enable the stream parameter' in err_msg:
                        # åˆ‡æ¢ä¸ºæµå¼
                        response = client.chat.completions.create(
                            model=api_model,
                            messages=[{"role": "user", "content": prompt}],
                            temperature=0.1,
                            stream=True
                        )
                        answer = ""
                        for chunk in response:
                            if hasattr(chunk, "choices") and chunk.choices and hasattr(chunk.choices[0], "delta"):
                                delta = chunk.choices[0].delta
                                if hasattr(delta, "content") and delta.content:
                                    answer += delta.content
                        answer = answer.strip().upper()
                        # æµå¼APIé€šå¸¸ä¸è¿”å›tokenè®¡æ•°ï¼Œä½¿ç”¨è¿‘ä¼¼ä¼°è®¡
                        # ä¼°ç®—è¾“å…¥tokenï¼ˆæ¯ä¸ªtokençº¦4ä¸ªå­—ç¬¦ï¼‰
                        input_tokens = len(prompt) // 4
                        # ä¼°ç®—è¾“å‡ºtoken
                        output_tokens = len(answer) // 4
                    else:
                        raise e
                else:
                    raise e
            # åªå–ç¬¬ä¸€ä¸ªå¤§å†™å­—æ¯
            match = re.search(r"[A-Z]", answer)
            if match:
                pred_index = ord(match.group(0)) - 65
                return pred_index == correct_index, input_tokens, output_tokens
            else:
                return False, input_tokens, output_tokens
        except Exception as e:
            retries += 1
            if retries > max_retries:
                print(f"\033[91mLLMè¯„å§”APIè°ƒç”¨å¤±è´¥: {e}\033[0m")
                return False, 0, 0


def batch_llm_mcq_judge(client, api_model, qa_list, max_workers=8, max_samples=None, token_budget=None, 
                        save_path=None, batch_size=10):
    """
    æ‰¹é‡è¯„å§”è¯„æµ‹ï¼Œæ”¯æŒå¤šçº¿ç¨‹ã€‚
    
    Args:
        client: OpenAI APIå®¢æˆ·ç«¯
        api_model: æ¨¡å‹åç§°
        qa_list: [(question, correct_answer, distractors), ...]
        max_workers: å¹¶å‘çº¿ç¨‹æ•°é‡
        max_samples: æœ€å¤§è¯„ä¼°æ ·æœ¬æ•°ï¼Œç”¨äºè°ƒè¯•æˆ–æ§åˆ¶æˆæœ¬
        token_budget: tokené¢„ç®—é™åˆ¶ï¼Œè¶…è¿‡åˆ™åœæ­¢è¯„ä¼°
        save_path: ç»“æœä¿å­˜è·¯å¾„ï¼Œç”¨äºæ–­ç‚¹é‡ä¼ 
        batch_size: æ‰¹æ¬¡ä¿å­˜å¤§å°ï¼Œæ¯å¤„ç†å¤šå°‘ä¸ªæ ·æœ¬ä¿å­˜ä¸€æ¬¡
        
    Returns:
        tuple: (æ­£ç¡®æ•°, æ€»æ•°, æ¯é¢˜æ˜¯å¦æ­£ç¡®åˆ—è¡¨, è¾“å…¥tokenæ¶ˆè€—, è¾“å‡ºtokenæ¶ˆè€—)
    """
    # è®¾å®šæ–­ç‚¹é‡ä¼ 
    start_idx = 0
    results = []
    input_tokens_total = 0
    output_tokens_total = 0
    
    # å¦‚æœæœ‰ä¿å­˜è·¯å¾„ï¼Œå°è¯•åŠ è½½ä¹‹å‰çš„ç»“æœ
    if save_path and os.path.exists(save_path):
        try:
            with open(save_path, 'r') as f:
                saved_data = json.load(f)
                start_idx = saved_data.get('processed_count', 0)
                results = saved_data.get('results', [])
                input_tokens_total = saved_data.get('input_tokens', 0)
                output_tokens_total = saved_data.get('output_tokens', 0)
                print(f"æ–­ç‚¹é‡ä¼ : ä»ç¬¬{start_idx}ä¸ªæ ·æœ¬ç»§ç»­ï¼Œå·²æœ‰{len(results)}ä¸ªç»“æœ")
        except Exception as e:
            print(f"åŠ è½½ä¿å­˜ç‚¹å¤±è´¥: {e}ï¼Œå°†ä»å¤´å¼€å§‹è¯„ä¼°")
            start_idx = 0
            results = []
            input_tokens_total = 0
            output_tokens_total = 0
    
    # å¦‚æœç»“æœä¸ºç©ºï¼Œåˆå§‹åŒ–ç»“æœåˆ—è¡¨
    if not results:
        results = [None] * len(qa_list)
    
    # å¦‚æœå·²ç»è¯„ä¼°å®Œæ‰€æœ‰æ ·æœ¬ï¼Œç›´æ¥è¿”å›
    if start_idx >= len(qa_list):
        correct_count = sum(1 for r in results if r is True)
        answered_count = sum(1 for r in results if r is not None)
        return correct_count, answered_count, results[:answered_count], input_tokens_total, output_tokens_total
    
    # é™åˆ¶æœ€å¤§æ ·æœ¬æ•°
    if max_samples and max_samples < len(qa_list) - start_idx:
        print(f"âš ï¸ å·²é™åˆ¶è¯„ä¼°æ•°é‡: æœ€å¤šå†è¯„ä¼°{max_samples}ä¸ªæ ·æœ¬")
        end_idx = start_idx + max_samples
    else:
        end_idx = len(qa_list)
    
    # äº’æ–¥é”ä¿æŠ¤tokenè®¡æ•°
    token_mutex = threading.Lock()
    
    # ç”¨äºæ˜¾ç¤ºtokenæ¶ˆè€—å’Œé¢„ä¼°æˆæœ¬çš„å‡½æ•°
    def show_token_stats(input_tokens, output_tokens):
        input_cost = input_tokens / 1000 * 0.002  # æ¯åƒtokenè¾“å…¥0.004å…ƒäººæ°‘å¸
        output_cost = output_tokens / 1000 * 0.08  # æ¯åƒtokenè¾“å‡º0.016å…ƒäººæ°‘å¸
        total_cost = input_cost + output_cost
        return f"æ¶ˆè€—tokens: è¾“å…¥{input_tokens}, è¾“å‡º{output_tokens}, æ€»è´¹ç”¨çº¦äººæ°‘å¸{total_cost:.2f}å…ƒ"
    
    def worker(idx, q, a, ds):
        nonlocal input_tokens_total, output_tokens_total
        is_correct, input_tokens, output_tokens = llm_mcq_judge(client, api_model, q, a, ds)
        with token_mutex:
            input_tokens_total += input_tokens
            output_tokens_total += output_tokens
        return idx, is_correct, input_tokens, output_tokens
    
    # ä¼˜åŒ–save_batch_resultså‡½æ•°ï¼Œæ·»åŠ å¯é€‰çš„éé˜»å¡é€‰é¡¹
    def save_batch_results(current_idx, non_blocking=False):
        nonlocal results, input_tokens_total, output_tokens_total
        if save_path:
            save_dir = os.path.dirname(save_path)
            if save_dir and not os.path.exists(save_dir):
                os.makedirs(save_dir, exist_ok=True)
            
            # å‡†å¤‡è¦ä¿å­˜çš„æ•°æ®
            save_data = {
                'processed_count': current_idx,
                'results': results[:current_idx],
                'input_tokens': input_tokens_total,
                'output_tokens': output_tokens_total,
                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
            }
            
            if non_blocking:
                # ä½¿ç”¨çº¿ç¨‹æ± è¿›è¡Œéé˜»å¡ä¿å­˜
                def _save_func():
                    try:
                        with open(save_path, 'w') as f:
                            json.dump(save_data, f, indent=2)
                    except Exception as e:
                        print(f"\033[93mè­¦å‘Š: ä¿å­˜æ•°æ®æ—¶å‡ºé”™: {e}\033[0m")
                
                threading.Thread(target=_save_func).start()
                print(f"\nä¿å­˜ç‚¹(å¼‚æ­¥): å·²å¤„ç†{current_idx}ä¸ªæ ·æœ¬ï¼Œ{show_token_stats(input_tokens_total, output_tokens_total)}")
            else:
                # å¸¸è§„é˜»å¡ä¿å­˜
                try:
                    with open(save_path, 'w') as f:
                        json.dump(save_data, f, indent=2)
                    print(f"\nä¿å­˜ç‚¹: å·²å¤„ç†{current_idx}ä¸ªæ ·æœ¬ï¼Œ{show_token_stats(input_tokens_total, output_tokens_total)}")
                except Exception as e:
                    print(f"\033[93mè­¦å‘Š: ä¿å­˜æ•°æ®æ—¶å‡ºé”™: {e}\033[0m")
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        to_process = [(idx, *qa_list[idx]) for idx in range(start_idx, end_idx)]
        futures = [executor.submit(worker, idx, q, a, ds) for idx, q, a, ds in to_process]
        correct_count = sum(1 for r in results[:start_idx] if r is True)
        total = len(futures)
        
        last_save_idx = start_idx
        
        with tqdm(total=total, desc="LLMè¯„å§”è¯„æµ‹ä¸­") as pbar:
            # ä½¿ç”¨æ›´é«˜æ•ˆçš„é˜Ÿåˆ—å¤„ç†æ–¹å¼
            completed = 0
            for fut in as_completed(futures):
                try:
                    idx, res, input_t, output_t = fut.result()
                    results[idx] = res
                    if res:
                        correct_count += 1
                    
                    # æ›´æ–°è¿›åº¦æ¡ä¿¡æ¯
                    completed += 1
                    pbar.update(1)
                    processed_count = start_idx + completed
                    pbar.set_postfix({
                        "æ­£ç¡®ç‡": f"{correct_count}/{processed_count} = {correct_count/processed_count:.2f}",
                        "Tokens": f"è¾“å…¥{input_tokens_total},è¾“å‡º{output_tokens_total}"
                    })
                    
                    # ä¿®æ”¹æ‰¹æ¬¡ä¿å­˜çš„è°ƒç”¨
                    if save_path and processed_count - last_save_idx >= batch_size:
                        # éé˜»å¡ä¿å­˜ï¼Œé¿å…è¿›åº¦æ¡å¡ä½
                        save_batch_results(processed_count, non_blocking=True)
                        last_save_idx = processed_count
                    
                    # æ£€æŸ¥æ˜¯å¦è¶…å‡ºtokené¢„ç®—
                    if token_budget and (input_tokens_total + output_tokens_total) >= token_budget:
                        print(f"\nâš ï¸ å·²è¾¾åˆ°tokené¢„ç®—ä¸Šé™: {input_tokens_total + output_tokens_total} >= {token_budget}")
                        for f in futures:
                            if not f.done():
                                f.cancel()
                        break
                except Exception as e:
                    print(f"\033[91må¤„ç†ç»“æœæ—¶å‡ºé”™: {e}\033[0m")
                    pbar.update(1)
        
        # ä¿®æ”¹æœ€åçš„ä¿å­˜é€»è¾‘
        # æœ€åä¿å­˜ä¸€æ¬¡
        if save_path and last_save_idx < start_idx + completed:
            # æœ€åä¸€æ¬¡ä¿å­˜ä½¿ç”¨é˜»å¡æ–¹å¼ç¡®ä¿æ•°æ®è¢«å†™å…¥
            save_batch_results(start_idx + completed, non_blocking=False)
    
    # è®¡ç®—æœ€ç»ˆç»“æœ
    correct_count = sum(1 for r in results if r is True)
    answered_count = sum(1 for r in results if r is not None)
    
    # æ˜¾ç¤ºtokenå’Œæˆæœ¬ç»Ÿè®¡
    print(f"\nğŸ“Š {show_token_stats(input_tokens_total, output_tokens_total)}")
    
    return correct_count, answered_count, results[:answered_count], input_tokens_total, output_tokens_total

# =====================
# å¤§æ¨¡å‹å¤šç»´åº¦æ‰“åˆ†æ‰¹é‡å·¥å…·
# =====================

def evaluate_distractor_multi_dim_llm(client, api_model, question, correct_answer, distractors, max_retries=2):
    """
    ä¸€æ¬¡æ€§è®©å¤§æ¨¡å‹å¯¹å››ä¸ªç»´åº¦æ‰“åˆ†ï¼Œè¿”å›å››ä¸ªåˆ†æ•°ã€‚
    ç»´åº¦ï¼šè²Œä¼¼åˆç†æ€§ã€ç»å¯¹é”™è¯¯æ€§ã€åŒºåˆ†åº¦ã€è¯Šæ–­ä»·å€¼ã€‚
    è‡ªåŠ¨å…¼å®¹æµå¼å’Œéæµå¼APIã€‚
    
    Args:
        client: OpenAI APIå®¢æˆ·ç«¯ 
        api_model: æ¨¡å‹åç§°
        question: é—®é¢˜æ–‡æœ¬
        correct_answer: æ­£ç¡®ç­”æ¡ˆ
        distractor: å¾…è¯„ä¼°çš„å¹²æ‰°é¡¹
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
    
    Returnsï¼š
        tuple: (dictæˆ–None, è¾“å…¥tokenæ¶ˆè€—, è¾“å‡ºtokenæ¶ˆè€—)
          dictçš„keyä¸ºç»´åº¦åï¼Œvalueä¸ºåˆ†æ•°ï¼ˆ1-5ï¼‰
    """
    if isinstance(distractors, str):
        distractors = [distractors]
        
    # æ„å»ºæ‰€æœ‰å¹²æ‰°é¡¹æ–‡æœ¬
    distractor_text = ""
    for i, d in enumerate(distractors, 1):
        distractor_text += f"Distractor {i}: {d}\n"
        
    prompt = f"""
Question: {question}
Correct answer: {correct_answer}
Distractor to evaluate: {distractor_text}

Please rate the following four dimensions on a scale of 1-5 (1 being the lowest, 5 being the highest), and ONLY return four numbers separated by commas without any explanation:
1. Plausibility (how reasonable the distractor seems)
2. Incorrectness (how clearly wrong the distractor is)
3. Distinctiveness (how different it is from the correct answer)
4. Diagnostic value (how well it reveals student misconceptions)

Return format example: 3,5,4,2
"""
    retries = 0
    while retries <= max_retries:
        try:
            # ä¼˜å…ˆå°è¯•éæµå¼
            input_tokens = 0
            output_tokens = 0
            try:
                response = client.chat.completions.create(
                    model=api_model,
                    messages=[{"role": "user", "content": prompt}],
                    temperature=0.1,
                    stream=False
                )
                content = response.choices[0].message.content.strip()
                if hasattr(response, 'usage'):
                    if hasattr(response.usage, 'prompt_tokens'):
                        input_tokens = response.usage.prompt_tokens
                    if hasattr(response.usage, 'completion_tokens'):
                        output_tokens = response.usage.completion_tokens
                    # å¦‚æœåªæœ‰total_tokensä½†æ²¡æœ‰ç»†åˆ†ï¼Œè¿›è¡Œä¼°ç®—
                    if hasattr(response.usage, 'total_tokens') and (input_tokens == 0 or output_tokens == 0):
                        input_tokens = int(response.usage.total_tokens * 0.8)  # ä¼°ç®—è¾“å…¥å 80%
                        output_tokens = response.usage.total_tokens - input_tokens  # ä¼°ç®—è¾“å‡ºå 20%
                else:
                    # è¿‘ä¼¼ä¼°è®¡tokenæ•°é‡
                    input_tokens = len(prompt) // 4
                    output_tokens = len(content) // 4
            except Exception as e:
                # æ£€æŸ¥æ˜¯å¦éœ€è¦æµå¼
                if hasattr(e, 'response') and hasattr(e.response, 'status_code') and e.response.status_code == 400:
                    err_msg = str(e)
                    if 'only support stream mode' in err_msg or 'please enable the stream parameter' in err_msg:
                        # åˆ‡æ¢ä¸ºæµå¼
                        response = client.chat.completions.create(
                            model=api_model,
                            messages=[{"role": "user", "content": prompt}],
                            temperature=0.1,
                            stream=True
                        )
                        content = ""
                        for chunk in response:
                            if hasattr(chunk, "choices") and chunk.choices and hasattr(chunk.choices[0], "delta"):
                                delta = chunk.choices[0].delta
                                if hasattr(delta, "content") and delta.content:
                                    content += delta.content
                        content = content.strip()
                        # æµå¼APIé€šå¸¸ä¸è¿”å›usageï¼Œè¿‘ä¼¼ä¼°è®¡
                        input_tokens = len(prompt) // 4
                        output_tokens = len(content) // 4
                    else:
                        raise e
                else:
                    raise e
            # ç”¨æ­£åˆ™æå–å››ä¸ª1-5çš„æ•°å­—
            match = re.search(r"([1-5])\s*,\s*([1-5])\s*,\s*([1-5])\s*,\s*([1-5])", content)
            if match:
                scores = [int(match.group(i)) for i in range(1, 5)]
                return {
                    "plausibility": {"score": scores[0]},
                    "incorrectness": {"score": scores[1]},
                    "distinctiveness": {"score": scores[2]},
                    "diagnostic_value": {"score": scores[3]}
                }, input_tokens, output_tokens
            else:
                print(f"\033[93mè­¦å‘Š: æœªèƒ½ä»LLMå“åº”ä¸­è§£æå‡ºå››ä¸ªåˆ†æ•°ã€‚å“åº”: {content[:100]}...\033[0m")
                return None, input_tokens, output_tokens
        except Exception as e:
            retries += 1
            if retries > max_retries:
                print(f"\033[91mLLMå¤šç»´åº¦æ‰“åˆ†APIè°ƒç”¨å¤±è´¥: {e}\033[0m")
                return None, 0, 0

def batch_llm_multi_dim_score(client, api_model, qa_list, max_workers=8, max_samples=None, token_budget=None, 
                              log_file=None, save_path=None, batch_size=10):
    """
    æ‰¹é‡å¤šç»´åº¦æ‰“åˆ†å·¥å…·
    
    Args:
        client: OpenAI APIå®¢æˆ·ç«¯
        api_model: æ¨¡å‹åç§°
        qa_list: [(question, correct_answer, distractorsåˆ—è¡¨), ...]
        max_workers: å¹¶å‘çº¿ç¨‹æ•°é‡
        max_samples: æœ€å¤§è¯„ä¼°æ ·æœ¬æ•°ï¼Œç”¨äºè°ƒè¯•æˆ–æ§åˆ¶æˆæœ¬
        token_budget: tokené¢„ç®—é™åˆ¶ï¼Œè¶…è¿‡åˆ™åœæ­¢è¯„ä¼°
        log_file: tokenæ¶ˆè€—æ—¥å¿—æ–‡ä»¶è·¯å¾„
        save_path: ç»“æœä¿å­˜è·¯å¾„ï¼Œç”¨äºæ–­ç‚¹é‡ä¼ 
        batch_size: æ‰¹æ¬¡ä¿å­˜å¤§å°ï¼Œæ¯å¤„ç†å¤šå°‘ä¸ªæ ·æœ¬ä¿å­˜ä¸€æ¬¡
        
    Returns:
        tuple: (æ¯é¢˜å››ç»´åº¦åˆ†æ•°å­—å…¸åˆ—è¡¨, è¾“å…¥tokenæ¶ˆè€—, è¾“å‡ºtokenæ¶ˆè€—)
    """
    # è®¾å®šæ–­ç‚¹é‡ä¼ 
    start_idx = 0
    results = []
    input_tokens_total = 0
    output_tokens_total = 0
    
    # å¦‚æœæœ‰ä¿å­˜è·¯å¾„ï¼Œå°è¯•åŠ è½½ä¹‹å‰çš„ç»“æœ
    if save_path and os.path.exists(save_path):
        try:
            with open(save_path, 'r') as f:
                saved_data = json.load(f)
                start_idx = saved_data.get('processed_count', 0)
                results = saved_data.get('results', [])
                input_tokens_total = saved_data.get('input_tokens', 0)
                output_tokens_total = saved_data.get('output_tokens', 0)
                print(f"æ–­ç‚¹é‡ä¼ : ä»ç¬¬{start_idx}ä¸ªæ ·æœ¬ç»§ç»­ï¼Œå·²æœ‰{len(results)}ä¸ªç»“æœ")
        except Exception as e:
            print(f"åŠ è½½ä¿å­˜ç‚¹å¤±è´¥: {e}ï¼Œå°†ä»å¤´å¼€å§‹è¯„ä¼°")
            start_idx = 0
            results = []
            input_tokens_total = 0
            output_tokens_total = 0
    
    # å¦‚æœç»“æœä¸ºç©ºï¼Œåˆå§‹åŒ–ç»“æœåˆ—è¡¨
    if not results:
        results = [None] * len(qa_list)
        
    # å¦‚æœå·²ç»è¯„ä¼°å®Œæ‰€æœ‰æ ·æœ¬ï¼Œç›´æ¥è¿”å›
    if start_idx >= len(qa_list):
        answered_count = sum(1 for r in results if r is not None)
        return results[:answered_count], input_tokens_total, output_tokens_total
    
    # é™åˆ¶æœ€å¤§æ ·æœ¬æ•°
    if max_samples and max_samples < len(qa_list) - start_idx:
        print(f"âš ï¸ å·²é™åˆ¶è¯„ä¼°æ•°é‡: æœ€å¤šå†è¯„ä¼°{max_samples}ä¸ªæ ·æœ¬")
        end_idx = start_idx + max_samples
    else:
        end_idx = len(qa_list)
    
    token_mutex = threading.Lock()  # ç”¨äºä¿æŠ¤total_tokensçš„äº’æ–¥é”
    
    # åˆ›å»ºæ—¥å¿—æ–‡ä»¶ç›®å½•
    if log_file:
        os.makedirs(os.path.dirname(log_file), exist_ok=True)
        with open(log_file, 'w') as f:
            f.write(f"{time.strftime('%Y-%m-%d %H:%M:%S')} - å¼€å§‹å¤šç»´åº¦è¯„åˆ†ï¼Œé¢„è®¡å¤„ç† {end_idx - start_idx} ä¸ªæ ·æœ¬\n")
        
    # è®°å½•èµ·å§‹æ—¶é—´ï¼Œç”¨äºå‘¨æœŸæ€§è®°å½•tokenæ¶ˆè€—
    start_time = time.time()
    last_log_time = start_time
    
    # ç”¨äºæ˜¾ç¤ºtokenç»Ÿè®¡å’Œé¢„ä¼°æˆæœ¬çš„å‡½æ•°
    def show_token_stats(input_tokens, output_tokens):
        input_cost = input_tokens / 1000 * 0.002  # æ¯åƒtokenè¾“å…¥0.004å…ƒäººæ°‘å¸
        output_cost = output_tokens / 1000 * 0.008  # æ¯åƒtokenè¾“å‡º0.016å…ƒäººæ°‘å¸
        total_cost = input_cost + output_cost
        return f"æ¶ˆè€—tokens: è¾“å…¥{input_tokens}, è¾“å‡º{output_tokens}, æ€»è´¹ç”¨çº¦äººæ°‘å¸{total_cost:.2f}å…ƒ"
    
    # ä¿å­˜æ‰¹æ¬¡ç»“æœçš„å‡½æ•°
    def save_batch_results(current_idx, non_blocking=False):
        nonlocal results, input_tokens_total, output_tokens_total
        if save_path:
            save_dir = os.path.dirname(save_path)
            if save_dir and not os.path.exists(save_dir):
                os.makedirs(save_dir, exist_ok=True)
            
            # å‡†å¤‡è¦ä¿å­˜çš„æ•°æ®
            save_data = {
                'processed_count': current_idx,
                'results': results[:current_idx],
                'input_tokens': input_tokens_total,
                'output_tokens': output_tokens_total,
                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
            }
            
            if non_blocking:
                # ä½¿ç”¨çº¿ç¨‹æ± è¿›è¡Œéé˜»å¡ä¿å­˜
                def _save_func():
                    try:
                        with open(save_path, 'w') as f:
                            json.dump(save_data, f, indent=2)
                    except Exception as e:
                        print(f"\033[93mè­¦å‘Š: ä¿å­˜æ•°æ®æ—¶å‡ºé”™: {e}\033[0m")
                
                threading.Thread(target=_save_func).start()
                print(f"\nä¿å­˜ç‚¹(å¼‚æ­¥): å·²å¤„ç†{current_idx}ä¸ªæ ·æœ¬ï¼Œ{show_token_stats(input_tokens_total, output_tokens_total)}")
            else:
                # å¸¸è§„é˜»å¡ä¿å­˜
                try:
                    with open(save_path, 'w') as f:
                        json.dump(save_data, f, indent=2)
                    print(f"\nä¿å­˜ç‚¹: å·²å¤„ç†{current_idx}ä¸ªæ ·æœ¬ï¼Œ{show_token_stats(input_tokens_total, output_tokens_total)}")
                except Exception as e:
                    print(f"\033[93mè­¦å‘Š: ä¿å­˜æ•°æ®æ—¶å‡ºé”™: {e}\033[0m")
    
    def worker(idx, q, a, d):
        nonlocal input_tokens_total, output_tokens_total, last_log_time
        res, input_t, output_t = evaluate_distractor_multi_dim_llm(client, api_model, q, a, d)
        with token_mutex:
            input_tokens_total += input_t
            output_tokens_total += output_t
            # æ¯60ç§’è®°å½•ä¸€æ¬¡tokenæ¶ˆè€—
            current_time = time.time()
            if log_file and (current_time - last_log_time > 60):
                with open(log_file, 'a') as f:
                    f.write(f"{time.strftime('%Y-%m-%d %H:%M:%S')} - {show_token_stats(input_tokens_total, output_tokens_total)}\n")
                last_log_time = current_time
        return idx, res, input_t, output_t
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        to_process = [(idx, *qa_list[idx]) for idx in range(start_idx, end_idx)]
        futures = [executor.submit(worker, idx, q, a, d) for idx, q, a, d in to_process]
        valid_results = sum(1 for r in results[:start_idx] if r is not None)
        total = len(futures)
        
        last_save_idx = start_idx
        
        with tqdm(total=total, desc="LLMå¤šç»´åº¦æ‰“åˆ†ä¸­") as pbar:
            # ä½¿ç”¨æ›´é«˜æ•ˆçš„é˜Ÿåˆ—å¤„ç†æ–¹å¼
            completed = 0
            for fut in as_completed(futures):
                try:
                    idx, res, input_t, output_t = fut.result()
                    results[idx] = res
                    if res:
                        valid_results += 1
                    
                    # æ›´æ–°è¿›åº¦æ¡ä¿¡æ¯
                    completed += 1
                    pbar.update(1)
                    processed_count = start_idx + completed
                    pbar.set_postfix({
                        "æœ‰æ•ˆæ¯”ä¾‹": f"{valid_results}/{processed_count} = {valid_results/processed_count:.2f}",
                        "Tokens": f"è¾“å…¥{input_tokens_total},è¾“å‡º{output_tokens_total}"
                    })
                    
                    # ä¿®æ”¹æ‰¹æ¬¡ä¿å­˜çš„è°ƒç”¨
                    if save_path and processed_count - last_save_idx >= batch_size:
                        # éé˜»å¡ä¿å­˜ï¼Œé¿å…è¿›åº¦æ¡å¡ä½
                        save_batch_results(processed_count, non_blocking=True)
                        last_save_idx = processed_count
                    
                    # æ£€æŸ¥æ˜¯å¦è¶…å‡ºtokené¢„ç®—
                    if token_budget and (input_tokens_total + output_tokens_total) >= token_budget:
                        print(f"\nâš ï¸ å·²è¾¾åˆ°tokené¢„ç®—ä¸Šé™: {input_tokens_total + output_tokens_total} >= {token_budget}")
                        for f in futures:
                            if not f.done():
                                f.cancel()
                        break
                except Exception as e:
                    print(f"\033[91må¤„ç†ç»“æœæ—¶å‡ºé”™: {e}\033[0m")
                    pbar.update(1)
        
        # ä¿®æ”¹æœ€åçš„ä¿å­˜é€»è¾‘
        # æœ€åä¿å­˜ä¸€æ¬¡
        if save_path and last_save_idx < start_idx + completed:
            # æœ€åä¸€æ¬¡ä¿å­˜ä½¿ç”¨é˜»å¡æ–¹å¼ç¡®ä¿æ•°æ®è¢«å†™å…¥
            save_batch_results(start_idx + completed, non_blocking=False)
    
    # è®°å½•æœ€ç»ˆçš„tokenæ¶ˆè€—
    if log_file:
        with open(log_file, 'a') as f:
            f.write(f"{time.strftime('%Y-%m-%d %H:%M:%S')} - æœ€ç»ˆ{show_token_stats(input_tokens_total, output_tokens_total)}\n")
            valid_count = sum(1 for r in results if r is not None)
            if valid_count > 0:
                effective_count = sum(1 for r in results if r is not None and r)
                f.write(f"æœ‰æ•ˆæ•°æ®: {effective_count}/{valid_count} = {effective_count/valid_count:.2f}\n")
    
    # æ˜¾ç¤ºtokenæ¶ˆè€—å’Œæˆæœ¬ä¼°è®¡
    print(f"\nğŸ“Š {show_token_stats(input_tokens_total, output_tokens_total)}")
    
    # åªè¿”å›æœ‰æ•ˆç»“æœ
    answered_count = sum(1 for r in results if r is not None)
    
    return results[:answered_count], input_tokens_total, output_tokens_total 